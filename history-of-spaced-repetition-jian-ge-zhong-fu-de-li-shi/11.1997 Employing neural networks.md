# 1997: Employing neural networks

## Neural Networks: Budding interest

In the mid-1980s, I read Michael Arbib's "*Brains, Machines and Mathematics*". It consolidated my view of the brain as an efficient computing machine.

For anyone with an interest in how the brain works, and this is almost everyone, neural networks are naturally fascinating. While studying computer science (1985-1990), I gained a new, computational perspective of the brain and the neural networks. As neural networks have an uncanny capacity to do their own modelling, it may seem natural to employ them to study memory data to provide answers on how memory works. However, neural networks have one major shortcoming, they do not easily share their findings. It is a bit like the problem with the brain itself, it can do magic things and yet it is hard to say what is actually happening inside. It is fun to write neural network software, I dabbled in that in 1989. It is a bit less fun to see a neural network in action.

The algebraic approach to [SuperMemo](https://supermemo.guru/wiki/SuperMemo) outstripped neural networks for two reasons: (1) my questions about memory always seemed too simple to involve neural networks, and (2) networks need data, which need learning, which needs an algorithm, which needs answering simple questions. In this chicken-and-egg race, my brain was always a step ahead of what I might figure out from available data with a neural network.

The superiority of the algebraic approach is obvious if you consider that the optimum interval can be found by just plotting a forgetting curve and employing regression to find the point where recall drops below 90%. This was even more extreme in my [1985 experiment](https://supermemo.guru/wiki/The_birthday_of_spaced_repetition:_July_31,_1985) where my "forgetting curve" was made of just 5 points. I was able to pick the one I just liked most. That was a pre-school exercise. No big guns needed.

In 1990, when working on the [model of intermittent learning](https://supermemo.guru/wiki/Search_for_a_universal_memory_formula), I came closest to employing neural networks. After a 7-hour-long discussion with [Murakowski](https://supermemo.guru/wiki/Janusz_Murakowski) on July 6, 1990, we concluded that a neural network could provide some answers. However, my computer was already churning data using algebraic hill-climbing methods. In essence, this is similar to feature extraction in neural networks. Once I got my answers, that motivation has been taken away. I did not need better computation. I needed better data.

Nevertheless, in the mid-1990s, we had more and more questions about the adaptability of the algorithm, and the possibility of employing neural networks. Those questions were primarily raised by those who do not understand [SuperMemo](https://supermemo.guru/wiki/SuperMemo) much. The model behind SuperMemo is simple, the optimization tools are simple, and they work pretty well to our satisfaction. The very first computational approach, [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2), is used to this day. Its adoption has actually reached [millions](https://supermemo.guru/wiki/Exponential_adoption_of_spaced_repetition). Human dissatisfaction with memory usually comes from unreasonable expectations that are built via school [curricula](https://supermemo.guru/wiki/Curricula). Unhappiness also comes from our poor ability to [formulate knowledge for healthy consumption](https://supermemo.guru/wiki/20_rules). This last habit is also perpetuated by the push for [cramming](https://supermemo.guru/wiki/Cramming) that we bring from school. SuperMemo algorithms cannot remedy that dissatisfaction with learning. They have always performed well, and the last 30 years delivered progress that can be measured mathematically, but which does not easily translate into an increase in the [pleasure of learning](https://supermemo.guru/wiki/Pleasure_of_learning). To many users, neural networks seemed like a magic pill that could change things for the better.

## Push for neural networks

In the 1990s, mail to [SuperMemo World](https://supermemo.guru/wiki/SuperMemo_World) often included hints that the neural network approach would be superior. Even a decade of use of [SuperMemo](https://supermemo.guru/wiki/SuperMemo) would not prevent a student from writing a set of false assertions:

> SuperMemo doesn't take different user abilities and needs into account. Instead, it assumes that every learner is a "bad learner". As such each learner will have the same repetition interval. Its underlying algorithm is hardwired, which might not be very efficient if you are a better/different learner. [...] Neural networks take into account that there exist very different types of learners who need different optimum repetition intervals. When reviewing new words and by "telling" the programme how well/bad they did the learner reveal more and more which type of learner they really are. After this feedback the programme is able to adapt and optimise the underlying repetition intervals if necessary

Those words indicate lack of understanding of [SuperMemo](https://supermemo.guru/wiki/SuperMemo). SuperMemo does not use a "bad student model". It only starts from shorter intervals before collecting first data about student's memory. The choice of shorter intervals comes from a faster convergence towards the optimum. In other words, SuperMemo adapts to individual students, and the "less than average student model" might be attributed to the starting point before any data is collected. Before training, neural network would also need to assume some model to make sure the scheduling is not entirely unpredictable.

In SuperMemo, the average student model is used only as an initial condition in the process of finding the model of the actual student's memory.

In a finite-dimensional trajectory optimization, convergence is fastest for a good initial state guess. Although it is not the case in SuperMemo due to its simple 3-dimensional nature of the function of [optimum intervals](https://supermemo.guru/wiki/Optimum_interval), in general case, the search for solutions may fail and the optimization will not work. Unlike the univalent matrices used in older SuperMemos for research purposes, a neural network algorithm would produce chaos without pre-training. This is why prior learning data are used to update the average or less-than-average student model used in SuperMemo for the maximum speed of convergence.

Note that this average student approach is even less significant in [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) due to the use of best-fit approximations for multiple parameters and functions in the learning process (e.g. item difficulty, stability increase function, etc.). This means that SuperMemo will always make the best of available data (using our current best knowledge of memory models).

The approximate shape of the [forgetting curve](https://supermemo.guru/wiki/Forgetting_curve) has been known for over a century now (see: [Error of Ebbinghaus forgetting curve](https://supermemo.guru/wiki/Error_of_Ebbinghaus_forgetting_curve)). SuperMemo collects precise data on the shape of forgetting curves for [items](https://supermemo.guru/wiki/Item) of different [difficulty](https://supermemo.guru/wiki/Difficulty) and different memory [stability](https://supermemo.guru/wiki/Stability). From forgetting curves, SuperMemo easily derives the [optimum interval](https://supermemo.guru/wiki/Optimum_interval). The data comes from only one student and each repetition contributes to the precision of the computation. In other words, with every minute you spend with SuperMemo, the program knows you better and better. Moreover, it knows you well enough after a month or two. You never need to worry about the efficiency of the algorithm.

There is an aura of mystique around neural networks. They are supposed to reveal hidden properties of the studied phenomena. It is easy to forget that networks can fail easily when they are fed with wrong information or with some vital information missing. This was the case with the only functional neural network used in [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition): [MemAid by David Calinski](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#David_Calinski_and_FullRecall).

The error in the design of MemAid network came from using Interval + Repetition Count on the input to represent the status of memory, while these two variables do not correspond with the [Stability](https://supermemo.guru/wiki/Stability) : [Retrievability](https://supermemo.guru/wiki/Retrievability) pair. Stability and Retrievability have been [proven necessary to represent the status of a long-term memory trace](https://supermemo.guru/wiki/Two_components_of_memory). In other words, the network does not get all the information it needs to compute the [optimum interval](https://supermemo.guru/wiki/Optimum_interval). A better design would code the entire [repetition history](https://supermemo.guru/wiki/Repetition_history), e.g. with the use of stability and retrievability variables. Full repetition history is needed to account for the [spacing effect](https://supermemo.guru/wiki/Spacing_effect) of massed presentation or a significant boost in stability for passing grades in delayed repetitions. Calinski's design would, however, meet basic requirements for learning in "optimum" intervals with few departures from the rules of spaced repetition (as much as [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2)).

## Is SuperMemo inflexible?

It is not true that SuperMemo is prejudiced while a neural network is not. Nothing prevents the optimization matrices in SuperMemo to depart from the memory model and produce an unexpected result. It is true that over years, with more and more knowledge of how memory works, the algorithm used in SuperMemo has been armed with restrictions and customized sub-algorithms. None of these were a result of a wild guess though. The progression of "prejudice" in SuperMemo algorithms is only a reflection of findings from the previous years. The same would inevitably affect any neural network implementation if it wanted to maximize its performance.

It is also not true that the original pre-set values of optimization matrices in SuperMemo are a form of prejudice. These are an equivalent of pre-training in a neural network. A neural network that has not been pre-trained will also be slower to converge onto the optimum model. This is why SuperMemo is "pre-trained" with the model of an average student.

The rate of interval increase is determined by the matrix of optimum intervals and is by no means constant. Moreover, the matrix of optimum intervals changes in time depending on the user's performance. You may have an impression of a fixed or rigid algorithm only after months or years of use (the speed of change is inversely proportional to the available learning data). This convergence reflects the invariability of the human memory system. It does not matter if you use the algebraic or neural approach to the optimization problem. In the end, you will arrive at the spaced repetition function that reflects the true properties of your memory. In that light, the speed of convergence should be held as a benchmark of the algorithm's quality. In other words, the faster the interval function becomes "fixed", the better.

Finally, there is another area where neural networks must either use the existing knowledge of memory models (i.e. carry a dose of prejudice) or lose out on efficiency. The experimental neural network SuperMemo, MemAid, as well as FullRecall have all exhibited an inherent weakness. The network achieves the stability when the intervals produce a desired effect (e.g. specific level of the measured forgetting index). Each time the network departs from the optimum model it is fed with a heuristic guess on the value of the optimum interval depending on the grade scored during repetitions (e.g. grade=5 would correspond with [130% of the optimum interval in SuperMemo NN](http://super-memory.com/english/ol/nn_train.htm) or [120% in MemAid](http://memaid.sourceforge.net/docs/ann.html)). The algebraic SuperMemo, on the other hand, can compute a difficulty estimate, use the accurate retention measurement, and produce an accurate adjustment of the value of the [stability increase](https://supermemo.guru/wiki/Stability_increase) matrix. In other words, it does not guess on the [optimal interval](https://supermemo.guru/wiki/Optimal_interval). It computes its exact value for that particular repetition. The adjustments to the memory matrices are weighted and produce a stable non-oscillating convergence. In other words, it is the memory model that makes it possible to eliminate the guess factor. With that respect, the algebraic SuperMemo is less prejudiced than the neural network SuperMemo.

## Futility of fine-tuning the spaced repetition algorithm

[Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) is a major step forward, however, many users will not notice the improvement and stick with the older algorithms. This perception problem led to the "[SM3+ myth](https://supermemo.guru/wiki/SM3%2B_myth)", which I tried to dispel in this [article](https://supermemo.guru/wiki/First_fast-converging_spaced_repetition_algorithm:_Algorithm_SM-5). At the same time, the value of the new algorithm for further progress in research is astronomical. In other words, there is a big dissonance between practical needs and theoretical needs. My words in an interview for Enter, 1994, still ring true:

> We have already seen that evolution speaks for SuperMemo, findings in the field of psychology coincide with the method, and that facts of molecular biology and conclusions coming from Wozniak's model seem to go hand in hand. Here is the time to see how the described mechanisms have been put to work in the program itself. In the course of repetitions, SuperMemo plots the forgetting curve for the student and schedules the repetition at the moment where the retention, i.e. proportion of remembered knowledge, drops to a previously defined level. In other words, SuperMemo checks how much you remember after a week and if you remember less than desired it asks you to make repetitions in intervals less than one week long. Otherwise, it checks the retention after a longer period and increases the intervals accordingly. A little kink to this simple picture comes from the fact that items of different difficulty have to be repeated at different intervals, and that the intervals increase as the learning process proceeds. Moreover, the optimum inter-repetition intervals have to be known for an average individual, and these must be used before the program can collect data about the real student. There must be obviously the whole mathematical apparatus involved to put the whole machinery at work. All in all, Wozniak says that there have been at least 30 days in his life when he had an impression that the algorithms used in SuperMemo have significantly been upgraded. Each of the cases seemed to be a major breakthrough. The whole development process was just a long succession of trials and errors, testing, improving, implementing new ideas, etc. Unfortunately, those good days are over. There have not been any breakthrough improvement to the algorithm since 1991. Some comfort may come from the fact that since then the software started developing rapidly providing the user with new options and solutions. Can SuperMemo then be yet better, faster, more effective? Wozniak is pessimistic. Any further fine-tuning of the algorithms, applying artificial intelligence or neural networks would be drowned in the noise of interference. After all, we do not learn in isolation from the world. When the program schedules the next repetition in 365 days, and the fact is recalled by chance at an earlier time, SuperMemo has no way of knowing about the accidental recollection and will execute the repetition at the previously planned moment. This is not optimal, but it cannot be remedied by improving the algorithm. Improving SuperMemo now is like fine tuning a radio receiver in a noisy car assembly hall. The guys at SuperMemo World are now less focused on science. In their view, after the scientific invention, the time has come for the social invention of SuperMemo.

## Dreger's Neural Network Project

On May 20, 1997, my net buddy from the pre-web [BBS](https://en.wikipedia.org/wiki/Bulletin_board_system) era, Bartek Dreger, came up with a great idea. He would also write his Master's Thesis about SuperMemo at Institute of Computer Science at Poznan University of Technology. That would be 8 years after my [own](https://supermemo.guru/wiki/Master's_Thesis), except he would use neural networks to see how they performed. Despite being nearly two decades younger, his plan was to try this project in the same great Weglarz operation research team I mention often elsewhere in this text. As early as in 1990, Dr Nawrocki came up with the idea to use neural networks to improve SuperMemo. The great mind of Prof. Roman Slowinski was to be the supervisor. This could really work.

By June 1997, another of my net buddies, Piotr Wierzejewski, joined the project. Then 3 more computer science students climbed aboard. It was a lovely team of five young brains with a combined age of 100. Soon the project was extended by the idea of on-line SuperMemo nicknamed: WebSorb (for the absorption of knowledge from the web). As it often happens in enthusiastic young teams, we started putting too much on the plate, and in the end, only a fraction of the goals has been attained. Only the on-line SuperMemo idea kept evolving and branching out in a meandering fashion with several mini-projects born and dying (e.g. [e-SuperMemo](http://super-memory.com/archive/english/ol/e-supermemo.htm), Quizer, Super-Memorizer, Memorathoner, etc.) until the emergence of [3GEMs](http://super-memory.com/archive/english/company/3gems.htm) that became [supermemo.net](https://supermemo.guru/wiki/SuperMemo.com) that ultimately evolved into today's [supermemo.com](http://supermemo.com/).

The greatest advantage of youth in similar projects is creativity and passion. The greatest obstacle is schooling, and later, other obligations, including having children. This fantastic brain trust fell victim to the ages old problem of school: converting a project born in passion into a project that became a school chore with deadlines, reports, tests, exams, and grades. As explained [here](https://supermemo.guru/wiki/SuperMemo_1.0_for_DOS_(1987)), SuperMemo was also born in that risky school environment. The key to success is the fight for freedom. Bondage destroys passions. The idea of [SuperMemo](https://supermemo.guru/wiki/SuperMemo) survived the pressure of schooling because of my [push for educational freedom](https://supermemo.guru/wiki/How_I_invented_perfect_schooling).

## Neural Network SuperMemo : Why memory model is essential for SuperMemo algorithms

Feature extraction proposed for [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition) neural networks is based on a well-proven existence of two components of long-term memory described [here](https://supermemo.guru/wiki/Two_component_model_of_memory).

[The two memory variables](https://supermemo.guru/wiki/Two_component_model_of_memory) are sufficient to represent the status of an [atomic memory trace](https://supermemo.guru/wiki/Complexity) in learning. Those variables make it possible to compute [optimum intervals](https://supermemo.guru/wiki/Optimum_interval), and account for the [spacing effect](https://supermemo.guru/wiki/Spacing_effect). The function of the [increase in memory stability](https://supermemo.guru/wiki/Stability_increase) for delayed repetitions is also known. For those reasons, a simple optimization algorithm makes it easy to determine optimum repetition spacing in [SuperMemo](https://supermemo.guru/wiki/SuperMemo). A neural network would need to code the full repetition history for each item and the most obvious coding choices are memory [stability](https://supermemo.guru/wiki/Stability) (S) and memory [retrievability](https://supermemo.guru/wiki/Retrievability) (R). In other words, the same basic assumptions must underlie the design of repetition spacing algorithms whether they are algebraic or neural. Needless to say, the algebraic solution is easy and fast. It converges fast. It requires no pre-training (memory model is encapsulated in the [matrix of optimum intervals](https://supermemo.guru/wiki/OF_matrix) or the [matrix of stability increase](https://supermemo.guru/wiki/Stability_increase)).

A neural network working with full repetition histories will produce the same outcomes as a hill-climbing algorithm employed in building [stability](https://supermemo.guru/wiki/Stability) of memory. Algorithmic hill-climbing is simply a better/faster tool for the job. It will carry the same limitations as neural networks, i.e. the answers will be as good as the questions posed.

## Neural Network SuperMemo: Design

With Bartek Dreger we designed a simple ANN system for handling the [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition) problem (Dec 1997). Note that this project would not be possible without the expertise of Dr Krzysztof Krawiec who was helpful in polishing the design:

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

The repetition spacing problem consists in computing optimum inter-repetition intervals in the process of human learning. The intervals are computed for individual pieces of information (later called items) and for a given individual. The entire input data are grades obtained by the student in repetitions of items in the learning process. This problem has until now be most effectively solved by means of a successive series of algorithms known commercially as     , Poland. Wozniak’s[model of memory](https://supermemo.guru/wiki/Neurostatistical_Model_of_Memory)used in developing the most recent version of the algorithm () cannot be considered as the ultimate algebraic description of human long-term memory. Most notably, the relationship between the complexity of the synaptic pattern and item difficulty is not well understood. More light on this relationship might be shed once a neural network is employed to provide adequate mapping between the memory status, grading and the item difficulty.

Using Wozniak’s model of two components of long-term memory we postulate that the following neural network solution might result in fast convergence and high repetition spacing accuracy.

The two memory variables needed to describe the state of a given engram are retrievability (R) and stability (S) of memory ([Wozniak, Gorzelanczyk, Murakowski, 1995](http://super-memory.com/english/2vm.htm)). The following equation relates R and S:

> (1) R=e-k/S*t
>
> where:
>
> - k is a constant
> - t is time

By using Eqn (1) we conclude about changes of retrievability in time at a given stability, as well as we can determine the optimum inter-repetition interval for given stability and given forgetting index.

The exact algebraic shape of the function that describes the change of stability upon a repetition is not known. However, experimental data indicate that stability usually increases from 1.3 to 3 times for properly timed repetitions and depends on item difficulty (the greater the difficulty the lower the increase). By providing the approximation of the optimum repetition spacing taken from experimental data as produced by optimization matrices of [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8), the neural network can be pre-trained to compute the [stability function](https://supermemo.guru/wiki/Search_for_a_universal_memory_formula):

> (2) Si+1=fs(R,Si,D,G)
>
> where:
>
> - Si is stability after the i-th repetition
> - R is retrievability before repetition
> - D is item difficulty
> - G is grade given in the i-th repetition

The stability function is the first function to be determined by the neural network. The second one is the item difficulty function with analogous input parameters:

> (3) Di+1=fd(R,S,Di,G)
>
> where:
>
> - Di is item difficulty approximation after the i-th repetition
> - R is retrievability before repetition
> - S is stability after the i-th repetition
> - G is grade given in the i-th repetition

Consequently, a neural network with four inputs (D, R, S and G) and two outputs (S and D) can be used to encapsulate the entire knowledge needed to compute inter-repetition intervals (see: [Implementation of the repetition spacing neural network](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Neural_Network_SuperMemo:_Implementation)).

The following approach will be taken in order to verify the feasibility of the aforementioned approach:

1. [Pretraining](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Pretraining) of the neural network will be done on the basis of approximated S and D functions derived from functions used in [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8) and experimental data collected thereof
2. Such a pretrained network will be implemented as a [SuperMemo Plug-In DLL](http://super-memory.com/archive/english/algplug.htm) that will replace standard [sm8opt.dll](http://super-memory.com/english/sm8opt.htm) used by SuperMemo 8 for Windows. The teaching of the network will continue in a real learning process in alpha testing of the neural network DLL. A procedure designed specifically for the purpose of the experiment will be used to provide cumulative results and a resultant neural network. The procedure will use neural networks used in alpha testing for training the network that will take part in beta-testing. The alpha-testing networks will be fed with a matrix of input parameters and their output will be used in as training data for the resultant network
3. In the last step, beta-testing of the neural network will be open to all volunteers over the Internet directly from the SuperMemo Website. The volunteers will only be asked to submit their resultant networks for the final stage of the experiment in which the ultimate network will be developed. Again, the beta-testing networks will all be used to train the resultant network. Future users of neural network SuperMemo (if the project appears successful) will obtain a network with a fair understanding of the human memory and able to further refine its reactions to the interference of the learning process with day-to-day activities of a particular student and particular study material.

The major problem in all spacing algorithms is the delay between comparing the output of the function of optimum intervals with the result of applying a given inter-repetition interval in practise. On each repetition, the state of the network from the previous repetition must be remembered in order to generate the new state of the network. In practise, this equates to storing an enormous number of network states in-between repetitions.

Luckily, Wozniak’s model implies that functions S and D are time-independent (interestingly, they are also likely to be user-independent!); therefore, the following approach may be taken for simplifying the procedure:

|         Time moment         |      T1       |          T2           |          T3           |
| :-------------------------: | :-----------: | :-------------------: | :-------------------: |
|          Decision           | I1N1O1=N1(I1) |     I2N2O2=N2(I2)     |     I3N3O3=N3(I3)     |
| Result of previous decision |               |     O*1E1=O*1-O1      |     O*2E2=O*2-O2      |
|   Evaluation for teaching   |               | O'1=N2(I1)E'1=O*1-O'1 | O'2=N3(I2)E'2=O*2-O'2 |

Where:

- Ei is an Error bound with Oi (see [error correction for memory stability](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Stability) and [error correction for item difficulty](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Difficulty))
- E'i is an Error bound with O'i
- Ii are input data at Ti
- Ni is the network state at Ti
- Oi is an output decision of Ni being given Ii, that is the decision after i-th repetition made at Ti
- O*i is an optimum output decision, that should be obtained at Ti instead of Oi; it can be computed from the grade and Oi (the grade indicates how Oi should have changed to obtain better approximation)
- O'i is an output decision of Ni+1 given Ii, that is the decision after i-th repetition that would be made at Ti+1
- Ti is time of the i-th repetition of a given item

The above approach requires only Ii-1 to be stored for each item between repetitions taking place at Ti-1 and Ti with substantial saving to the amount of data stored during the learning process (E'i is as valuable for training as Ei). This way the proposed solution is comparable for its space complexity with the [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8)! Only one (current) state of the neural network has to be remembered throughout the process.

These are the present implementation assumptions for the discussed project:

- neural network: unidirectional, layered, with resilient back-propagation; an input layer with four neurons, an output layer with two neurons, and two hidden layers (15 neurons each)
- item difficulty interpretation: same as in [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8), i.e. defined by [A-Factor](https://supermemo.guru/wiki/A-Factor)
- each item stores: Last repetition date, [Stability](https://supermemo.guru/wiki/Stability) (at last repetition), [Retrievability](https://supermemo.guru/wiki/Retrievability) (at last repetition), [Item difficulty](https://supermemo.guru/wiki/Complexity), Last grade
- default [forgetting index](https://supermemo.guru/wiki/Forgetting_index): 10%
- [network DLL input](http://super-memory.com/archive/english/algplug.htm) (at each repetition): item number and the current grade
- [network DLL output](http://super-memory.com/archive/english/algplug.htm) (at each repetition): next repetition date
- neural network DLL implementation language: C++
- neural network DLL shell, SuperMemo 98 for Windows (same as the 32-bit [SM8OPT.DLL](http://super-memory.com/english/sm8opt.htm) shell)

## Neural Network SuperMemo: Implementation

The network has practically been given the [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition) algorithm on a silver platter. Its only role would be to fine-tune the performance over time. This is exactly what all [SuperMemo](https://supermemo.guru/wiki/SuperMemo) algorithms do as of [Algorithm SM-5](https://supermemo.guru/wiki/Algorithm_SM-5). In that sense, the design did not ask the network for a discovery. It asked for improvements upon the discovery. The model was wired in into the design.

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

**Basic assumption**

The state of memory will be described with only two variables: retrievability (R) and stability (S) ([Wozniak, Gorzelanczyk, Murakowski, 1995](http://super-memory.com/english/2vm.htm)). The following equation relates R and S:

> (1) R=e-k/S*t
>
> where:
>
> - k is a constant
> - t is time

For simplicity, we will set k=1 to univocally define stability.

**Input and output**

The following functions are to be determined by the network:

> (2) Si+1=fs(R, Si, D, G)
>
> (3) Di+1=fd(R, S, Di, G)

The neural network is supposed to generate stability (S) and item difficulty (D) on the output given R, S, D and G on the input:

> (4) (Ri, Si, Di, Gi) => (Di+1,Si+1)
>
> where:
>
> - Ri is retrievability before the i-th repetition
> - Si is stability before the i-th repetition
> - Si+1 is stability after the i-th repetition
> - Di is item difficulty before the i-th repetition
> - Di+1 is item difficulty after the i-th repetition
> - Gi is grade given in the i-th repetition

**Error correction for difficulty D**

Target difficulty will be defined as in [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8) as the ratio between second and first intervals. The [neural network plug-in](http://super-memory.com/archive/english/algplug.htm) (NN.DLL) will record this value for all individual items and use it in training the network:

> (5) Do=I2/I1
>
> where:
>
> - Do is guiding difficulty used in error correction (the higher the Do, the less the difficulty)
> - I1 is the first optimum interval computed for the item in question (same for all items)
> - I2 is the second optimum interval computed for the item

Important! The optimum intervals I1 and I2 are not the ones proposed by the network before its verification but the ones used in error correction after the proposed interval had already been executed and verified (see [error correction for stability S](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Stability))!

The initial value of difficulty will be set to 3.5, i.e. D1=3.5. This is for similarity with Algorithm SM-8 only. As initial difficulty is not known, it cannot be used to determine the first interval. After scoring the first grade the error correction is still impossible due to the fact that second optimum interval is not known. Once it is known, Do can be used for error correction of D on the output.

To avoid convergence problems in the network, the following formula will be used to determine the correct output on D:

> (6) Dopt=0.9*Di+0.1*Do
>
> where:
>
> - Dopt is difficulty used in error correction after the i-th repetition
> - Di is difficulty before the i-th repetition
> - Do is guiding difficulty from Eqn (5)

The convergence factor of 0.9 in Eqn (6) is arbitrary and may change depending on the network performance.

**Error correction for stability S**

The following formula, derived from Eqn (1) for forgetting index equal 10% and k=1, makes it easy to convert stability and the optimum interval: I=-ln(0.9)*S

In the optimum case, the network should generate the requested forgetting index for each repetition. Variable forgetting index can easily be used once the stability S is known (see Eqn (1)). For simplicity then we will use forgetting index equal 10% in further analysis.

To accelerate the convergence, the network will measure forgetting index for 25 classes of repetitions. These classes are set by (1) five difficulty categories: 1-1.5, 1.5-2.5, 2.5-3.5, 3.5-5, and over 5, and (2) five interval categories: 1-5, 5-20, 20-100, 100-500 and over 500 days. We will denote the forgetting index measurements for these categories as FI(Dm,In). Additionally, the overall forgetting index FItot will be measured and used in stability error correction.

The ultimate goal is to reach the forgetting index of 10% in all categories. The following formula will be used in error correction for stability:

> (7) FIopt(m,n)=(10*FItot+Cases(m,n)*FI(m,n))/(10+Cases(m,n))
>
> where:
>
> - FIopt(m,n) is forgetting index used in error correction after a repetition belonging to category (m,n)
> - FItot is the overall forgetting index measured in repetitions
> - Cases(m,n) is the number of repetition cases used to measure the forgetting index in category (m,n)

The formula in Eqn (7) is supposed to shift the weight on error correction from the overall forgetting index to forgetting index recorded in given categories as soon as the number of cases in individual categories increases. Obviously, for Cases(m,n)=0, we have FIopt(m,n)=FItot. For Cases(m,n)=10 the weights for overall and category FI balance, and for a large number of cases, FIopt(m,n) is approaching FI(m,n).

The following table illustrates the assumed relationship between FIopt(m,n), grades and the interval correction applied:

|     Grade      |       0       |       1       |       2       |       3       |       4       |       5       |
| :------------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: |
| FIopt(m,n)>10% |      40%      |      60%      |      80%      | no correction | no correction | no correction |
| FIopt(m,n)=10% | no correction | no correction | no correction | no correction | no correction | no correction |
| FIopt(m,n)<10% | no correction | no correction | no correction |     110%      |     120%      |     130%      |

In [SuperMemo](https://supermemo.guru/wiki/SuperMemo), grades less than 3 are interpreted as forgetting, while grades equal 3 or more are understood as sufficient recall. That is why no correction is used for passing grades in case of satisfactory FI, and no correction is used for failing grades if FI is greater than requested. An exemplary correction for an excessive forgetting rate and grade=2 for applied interval of 10 days would be 80%. Consequently, the network will be instructed to assume Interval=8 as correct. Correct stability would then be derived from S=-8/ln(0.9) and used in error correction. The values of interval corrections are arbitrary but shall not undermine the convergence of the network. In case of unlikely stability problems, the corrections might be reduced (note that the environmental noise in the learning process will dramatically exceed the impact of ineffectively choosing the correction factors!). Similar corrections used to be applied in successive SuperMemo algorithms with encouraging results.

**Border conditions**

The following additional constraints will be imposed on the neural network to accelerate the convergence:

- interval increase in two successive repetition must be at least 1.1 (consequently, difficulty cannot be less than 1.1)
- interval increase cannot surpass 8 after the first repetition, and 4 in later repetitions
- the first interval must fall between 1 and 40 days
- difficulty measure cannot exceed 8

These conditions will not prejudice the network as they have been proven beyond reasonable doubt as true in the practice of using SuperMemo and its implementations over the last ten years.

**Pretraining**

In the pretraining stage, the following form of Eqns (2) and (3) will be used:

> (8) Di+1:=Di+(0.1-(5-G)*(0.08+(5-G)*0.02))
>
> (9) Si+1:=Si*Di*(0.5+1/i)

With D1=3.5 and S1=-3/ln(0.9).

Eqn (8) has been derived from [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2) (see E-Factor equation). Eqn (9) has been roughly derived from Matrix OF in [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8). D1=3.5 corresponds with the same setting in Algorithm SM-8. S1=-3/ln(0.9) corresponds with the first interval of 3 days and forgetting index 10%. The value of 3 days is close to an average across a wide spectrum of students and difficulty of the learning material.

Pretraining will also use border conditions mentioned in the previous paragraph.

There were multiple problems with the neural network, implementation, bugs, convergence, interference, and the like. The only way to effectively study the network was to plug it in real SuperMemo and see how it works on real data. I came up with an idea of plug-in algorithms in a DLL. We could study algorithmic variants in the same shell. We tried out [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2), [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8) and now a neural network was to do the same. Unfortunately, that DLL implementation proved a step too far. Once the enthusiastic kids graduated, they soon dispersed, found jobs elsewhere, got married, and I never had a chance to try the plug-in in my own learning in my favorite shell, which was SuperMemo 9 at that time (aka SuperMemo 98).

Neural network SuperMemo was a student project with a sole intent to verify the viability of neural networks in spaced repetition. Needless to say, neural networks are a viable tool. Moreover, all imaginable valid optimization tools, given sufficient refinement, are bound to produce similar results to those currently accomplished by SuperMemo. In other words, as long as the learning program is able to quickly converge to the optimum model and produce the desired level of knowledge retention, the optimization tool used to accomplish the goal is of secondary importance.

Considering the number of problems at earlier stages, I doubt that successful plug-in would change my thinking about neural networks. I am a programmer and a tinkerer, I like to see what I create. Neural network appeared too black boxy to me. As for the team, they are all successful in their careers today. The kids have contributed to some other SuperMemo efforts later on. Youth is creative, youth is unpredictable, and I am glad we took on the project.

## David Calinski and FullRecall

David Calinski (b. 1981) was one of the early youthful SuperMemo enthusiasts in the 1990s. He showed rich interest in accelerated learning, psychology, psychiatry, and beyond.

I quickly recognized his talents and was hoping to recruit him in some SuperMemo projects, incl. SuperMemo for Linux, however, many a genius like to walk alone. At some point, he switched from SuperMemo to his own application (FullRecall, see later), and from that point on, he would not abandon his project.

Our discussions about neural networks started in 2001. David was a fan of SuperMemo, however, he also admitted to have never truly studied the algorithm. This led to a criticism:

> I don't know the exact details of SM algorithm(s) (I never was much interested in it), but important here is the main idea. Algorithm in SM gets some data (e.g. number of repetitions, difficulty of item, current grade, etc. etc.) and returns next optimal interval it calculated. This algorithm, even if it's "smart" and corrects itself somehow, will be still dumb - it won't correct itself more than was designed for.

He is right, [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) is inherently bound to the [two component model of long-term memory](https://supermemo.guru/wiki/Two_component_model_of_long-term_memory), however, this is a happy marriage. The bond can only be broken by counter-evidence that hasn't come in three decades thus far.

David's stance is entirely justifiable. It is all about modelling and prior knowledge. For David, [Algorithm SM-8](https://supermemo.guru/wiki/Algorithm_SM-8) was complex. Neural networks seem like a simple way to take away the complexity. To me, my own algorithm is as simple as the multiplication table. That modelling difference often leads to cognitive divergence and this is a good thing. Without those differences, we would know much less about neural networks in spaced repetition today!

I wrote to David in 2004: *"Further improvements to the algorithm used in SuperMemo are not likely to result in further acceleration of learning. However, there is still scope for improvement for handling unusual cases such as dramatically delayed repetitions, massed presentation, handling items whose contents changed, handling semantic connections between items, etc. Interestingly, the greatest progress in the algorithm is likely to come from a better definition of the model of human long-term memory. In particular, the function describing changes in memory stability for different levels of retrievability is becoming better understood. This could dramatically simplify the algorithm. Simpler models require fewer variables and this simplifies the optimization. The algorithm based on stability and retrievability of memory traces could also result in better handling of items with low retrievability. However, as unusual item cases in the learning process form a minority, and testing a new algorithm would take several years, it is not clear if such an implementation will ever be undertaken"*.

David developed his own neural network, MemAid. Later he converted it into a commercial product. The move from free to commercial was hard as users tend to prefer a drop in prices, for obvious reasons. Despite all ups and downs, David persisted, and his DIY tinkerer and passion for science and programming always gave him an upper hand. Like Anki, he tried to keep his program cross-platform which imposed some limits and demands on simplicity. In his words: *"I love speed and lack of borders, lack of dependency on just one solution, system, computer, etc."*

Today FullRecall is free. See the [changelong](http://fullrecall.com/changelog).

[![ANN interval distribution (in FullRecall)](https://supermemo.guru/images/f/f1/ANN_interval_distribution.jpg)](https://supermemo.guru/wiki/File:ANN_interval_distribution.jpg)

> ***Figure:** Interval distribution in FullRecall. Repetitions scheduled with the help of a neural network*

The open-source [MemAid project](http://memaid.sourceforge.net/) closed in 2006, but FullRecall continued. So did another project inspired by MemAid: [Mnemosyne](https://supermemo.guru/wiki/Mnemosyne). Mnemosyne, however, opted for their own version of [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2). To this day, Mnemosyne generates data that can be used by spaced repetition enthusiasts or researchers at [The Mnemosyne Project](https://mnemosyne-proj.org/).

Like Calinski, Peter Bienstman is skeptical of newer algorithms: *"SuperMemo now uses SM-11. However, we are a bit skeptical that the huge complexity of the newer SM algorithms provides for a statistically relevant benefit. But, that is one of the facts we hope to find out with our data collection."*

*"Statistically relevant benefit"* depends on the criteria. For users, the actual algorithm may be secondary. For research, [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) is a goldmine (as much as data that all programs like Mnemosyne can generate).

## Why is the neural network in FullRecall flawed?

[The two memory variables](https://supermemo.guru/wiki/Two_component_model_of_memory) are both necessary and sufficient to represent an [atomic memory](https://supermemo.guru/wiki/Atomic_memory) in [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition). Moreover, the two variables can be used to account for the [spacing effect](https://supermemo.guru/wiki/Spacing_effect) in massed presentation. They can also explain the benefit of high [forgetting index](https://supermemo.guru/wiki/Forgetting_index) for long-term [retention](https://supermemo.guru/wiki/Retention) as discussed here. Those two variables of long-term memory which we named: [stability](https://supermemo.guru/wiki/Stability) and [retrievability](https://supermemo.guru/wiki/Retrievability) are necessary to represent the status of memory. Any neural network that wants to find patterns in the relationship between spacing and recall must receive the full status of memory on its input otherwise it won't ever compute the optimum spacing. That status may have a form of the full [history of repetitions](https://supermemo.guru/wiki/History_of_repetitions). It may also be the [stability](https://supermemo.guru/wiki/Stability) : [retrievability](https://supermemo.guru/wiki/Retrievability) pair (if it can be computed). It may also be any other code over the history of repetitions from which the status of memory can be computed.

The design of the FullRecall network does not meet those criteria:

- input: last_interval_computed_by_ann [0-2048 days] (zero if this is not a review, but a first presentation)
- input: real_interval_since_last_review [0-2048 days] (same comment as above)
- input: number_of_repetitions_of_an_item_so_far [0-128]
- input: current_grade [0-5, 5 is the best]
- output that ANN gives us: new_interval [0-2048]

Neither interval nor repetitions count can reflect memory stability or retrievability. You can obtain high repetition counts in massed presentation subject to spacing effect with a negligible increase in memory stability. At the same time, long intervals for suboptimum schedules may result in low values for both stability and retrievability. In short, for the same interval, the status of memory will depend on the distribution of repetitions in time.

This can be shown with an example: for 10 repetitions, and 1000 days, 9 repetitions in 9 days combined with 991 day interval will produce stability approaching zero (assuming no interference). At the same time, for the same pair of inputs, optimally spaced repetition can bring retrievability of nearly 100% and stability that allows of optimum intervals close to 1000 days.

The only scenario where the network might perform well is where the user adheres precisely to the optimum spaced repetition schedule. This, in turn, can only come from a network that has been pre-trained, e.g. with [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2). In this scenario, the network will be unstable and won't converge on the optimum due to the fact that all departures from the optimum schedule, incl. those caused by network error will shift the state of the network away from the original state in which it was still able to compute memory status from its inputs.

Stability and retrievability are sufficient in the idealized case for a unitary monosynaptic association. In real life, the semantic network involved in the association is likely to involve a number of such ideal unitary memories. This is why SuperMemo uses the concept of absolute item difficulty. In [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17), the absolute item difficulty is determined by the maximum increase in memory [stability](https://supermemo.guru/wiki/Stability) for the first optimally scheduled review at the default [forgetting index](https://supermemo.guru/wiki/Forgetting_index) of 10%. The FullRecall network does not receive any reliable measure of item difficulty either. This will compound the network's inefficiency.

The FullRecall network is said to work pretty well, according to user reports. In the light of the present analysis, the network might employ well-chosen boundary conditions, however, this would be equivalent to returning to [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2) employed in older versions of SuperMemo. Needless to say, that old SuperMemo algorithm is more biased and less plastic than newer matrix-based algebraic versions employed in SuperMemo.

If the FullRecall network is pre-trained, e.g. with the help of [Algorithm SM-2](https://supermemo.guru/wiki/Algorithm_SM-2), and the student sticks rigorously to his or her repetition, the network might work ok as the interval correlates well with memory stability, esp. if the information is enhanced by the number of repetitions. However, without appropriate boundary conditions, in [incremental reading](https://supermemo.guru/wiki/Incremental_reading), the network would certainly fail as it might receive false memory status information. Depending on the scenario, the same Repetitions : Interval pair may occur for Stability=0 and for maximum stability corresponding with lifetime memories. Similarly, the retrievability may also vary in the 0-1 range for the same input pair in the network. For example, frequent subset review before an exam followed by a longer break in learning (e.g. caused by overflow) may correspond with very low stability and retrievability despite providing the same input as a correctly executed series of spaced reviews in the same period (with high stability and retrievability above 0.9). In [incremental reading](https://supermemo.guru/wiki/Incremental_reading), overload, auto-postpone, item advance, subset review, and spacing effect would be invisible to the network.

Assuming good design, the flaws of FullRecall will then only show in intermitted learning, which may trigger boundary conditions. It should not detract from the value of the software itself. It is only to emphasize that neural network design is not easy, and may turn out inferior. It may even be inferior in comparison to older, allegedly less plastic, algebraic algorithms.

In short, FullRecall inputs do not reflect all necessary information needed for computing optimum intervals. In particular, repetition count is a very poor measure of memory stability or retrievability. A better approach would be to code the entire[history of repetitions](https://supermemo.guru/wiki/History_of_repetitions) or compute the status of memory with the use of [stability](https://supermemo.guru/wiki/Stability) and [retrievability](https://supermemo.guru/wiki/Retrievability) variables. Both stability and retrievability must be computable from the network input.

## Future of neural networks in SuperMemo

In our discussions with Calinski (in 2001), I summarized my reservations and vowed to continue on the same old "conservative" path. 17 years later, I am glad. There has not been much progress in the area of employing neural networks in spaced repetition. It might be the fact that SuperMemo itself is an inhibitor of progress. In the meantime, however, [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) has revealed further potential for improvements in [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition) and understanding human memory. Time permitting, there will still be progress.

SuperMemo will continue with its algebraic algorithms for the following reasons:

- **Known model**: Neural networks are superior in cases where we do not know the underlying model of the mapped phenomena. The model of forgetting is well-known and makes it easy to fine-tune the algebraic optimization methods used in computing inter-repetition intervals. The well-known model also makes SuperMemo resistant to unbalanced datasets, which might plague neural networks, esp. in initial stages of learning. Last but not least, the validity of the [two component model of memory](https://supermemo.guru/wiki/Two_component_model_of_memory) has been proven in many ways and giving up the model while designing the network, in the name of stemming prejudice, would be wasteful. Such approach may have a research value only
- **Overlearning**: Due to case-weighted change in array values, optimization arrays used in SuperMemo are not subject to "overlearning". No pretraining is needed, as the approximate shape of the function of optimal intervals is known in advance. There is no data representation problem, as all kinky data input will be "weighed out" in time
- **Equivalence**: Mathematically speaking, for continuous functions, n-input networks are equivalent to n-dimensional arrays in mapping functions with n arguments, except for the "argument resolution problem". The scope of argument resolution problem, i.e. the finite number of argument value ranges, is strongly function dependent. A short peek at the optimization arrays displayed by SuperMemo indicates that the "argument resolution" is far better than what is actually needed for this particular type of function, esp. in the light of the substantial "noise" in data. Hill-climbing algorithms used in SuperMemo are reminiscent of the algorithms aimed at reweighing the networks
- **Research**: The use of matrices in SuperMemo makes it easy to see "memory in action". Neural networks are not that well-observable. They do not effectively reveal their findings. You cannot see how a single forgetting curve affects the function of optimum intervals. This means that the black-box nature of neural networks makes them less interesting as a memory research tool
- **Convergence**: The complexity of the algorithm does not result from the complexity of the memory model. Most of the complexity comes from the use of tools that are supposed to speed up the convergence of the optimization procedure without jeopardizing its stability. This fine-tuning is only possible due to our good knowledge of the underlying memory model, as well as actual learning data collected over years that help us precisely determine best approximation function for individual components of the model
- **Forgetting curve**: The only way to determine the [optimum interval](https://supermemo.guru/wiki/Optimum_interval) for a given [forgetting index](https://supermemo.guru/wiki/Forgetting_index) is to know the (approximate) forgetting curve for a given [difficulty](https://supermemo.guru/wiki/Difficulty) class and memory [stability](https://supermemo.guru/wiki/Stability). If a neural network does not attempt to map the forgetting curve, it will always oscillate around the value of the optimum interval (with good grades increasing that value, and bad grades decreasing it). Due to data noise, this is only a theoretical problem. However, it illustrates the power of the symbolic representation of stability-retrievability-difficulty-time relationship instead of a virtually infinite number of possible forgetting curve data sets. If the neural network does not use a weighted mapping of the forgetting curve, it will never converge. In other words, it will keep oscillating around the optimum model. If the neural network weighs in the status history and/or employs the forgetting curve, it will take the same approach as the present SuperMemo algorithm, which was to be obviated by the network in the first place

In sum, neural networks could be used to compute the intervals, but they do not seem to be the best tool in terms of computing power, research value, stability, and, most of all, the speed of convergence. When designing an optimum neural network, we run into similar difficulties as in designing the algebraic optimization procedure. In the end, the same boundary conditions that are set in "classic" [SuperMemo](https://supermemo.guru/wiki/SuperMemo) will also show up, sooner or later, in the network design (as can be seen in: [Neural Network SuperMemo](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Neural_Network_SuperMemo:_Design)).

As with all function approximations, the choice of the tool, and minor algorithmic adjustments can make a world of difference in the speed of convergence and the accuracy of mapping. Neural networks could find use in mapping the lesser known accessory functions that are used to speed up the convergence of the algebraic algorithm. For example, to this day, [item difficulty](https://supermemo.guru/wiki/Complexity) estimate problem has not been fully cracked. We simply tell users to keep their knowledge simple, which is a universal recommendation from any educator aware of mnemonic limits of human memory.