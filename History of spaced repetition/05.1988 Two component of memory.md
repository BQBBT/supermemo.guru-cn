# 1988: Two component of memory

[Two-component model of long-term memory](https://supermemo.guru/wiki/Two-component_model_of_long-term_memory) lays at the foundation of [SuperMemo](https://supermemo.guru/wiki/SuperMemo), and is expressed explicitly in [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17). It differentiates between how [stable](https://supermemo.guru/wiki/Stability) knowledge is in long term memory storage, and how [easy it is to retrieve](https://supermemo.guru/wiki/Retrievability). This remains a little known and quintessential fact of the theory of learning that one can be fluent and still remember poorly.

Fluency is a poor measure of learning in the long term

## Components of long-term memory

For many years, memory researchers used the term *strength of memory*. It was supposed to reflect how well things are remembered.

My work over [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition) in the mid 1980s lead to a quick realization that one variable, *strength*, is not enough to describe the status of a piece of knowledge stored in long-term memory. We need two variables that separate the strength from the ease with which a fact can be retrieved from memory.

I concluded that the status of a piece of knowledge stored in long-term memory can be described by two variables that I named: [stability](https://supermemo.guru/wiki/Stability) and [retrievability](https://supermemo.guru/wiki/Retrievability). Stability of memory tells you how long the memory can last in the storage. Retrievability tells you how easy it is to recall a piece of knowledge. Those two variables used to be conflated in memory research.

Amazingly, three decades after my observations, the idea of two components of memory is still to permeate mainstream memory research. In 2018, most of papers ignore the separation between the two variables and still rely on a single concept of *memory strength*. This leads to monumental confusion and slow progress in research.

## Origins of the two component model of memory

I first described the idea of the [two components of long-term memory](https://supermemo.guru/wiki/Two_component_model_of_memory) in a paper for my computer simulations class on Jan 9, 1988. In an unrelated line of reasoning, in the same paper, I concluded that different circuits must be involved in [declarative](https://supermemo.guru/wiki/Declarative_learning) and [procedural learning](https://supermemo.guru/wiki/Procedural_learning).

If you pause for a minute, the whole idea of the two components should be pretty obvious. If you take two [memories](https://supermemo.guru/wiki/Item) right after a review, one with a short [optimum interval](https://supermemo.guru/wiki/Optimum_interval), and the other with a long optimum interval, the memory status of the two must differ. Both can be recalled perfectly (maximum [retrievability](https://supermemo.guru/wiki/Retrievability)) and they also need to differ in how long they can last in memory (different [stability](https://supermemo.guru/wiki/Stability)). I was surprised I could not find any literature on the subject. However, if the literature has no mention of the existence of the [optimum interval](https://supermemo.guru/wiki/Optimum_interval) in [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition), this seemingly obvious conclusion might be hiding behind another seemingly obvious idea: the [progression of increasing interval in optimally spaced review](https://supermemo.guru/wiki/Birth_of_SuperMemo). This is a lovely illustration how human progress is incremental and agonizingly slow. We are notoriously bad at thinking out of the box. The darkest place is under the candlestick. This weakness can be broken with an explosion of communication on the web. I advocate less [peer review](https://supermemo.guru/wiki/Peer_review), and more bold hypothesizing. I speak of a fantastic example coming from [Robin Clarke's paper in reference to Alzheimer's](https://supermemo.guru/wiki/Bad_learning_contributes_to_Alzheimer's). Strict peer review is reminiscent of [Prussian schooling](https://supermemo.guru/wiki/Prussian_education_system): in the quest for perfection, we lose our creativity, then humanity, and ultimately the pleasure of life.

When I first presented my ideas to my teacher Dr Katulski on Feb 19, 1988, he was not too impressed, but he gave me a pass for computer simulations credit. Incidentally, a while later, Katulski became one of the first users of [SuperMemo 1.0 for DOS](https://supermemo.guru/wiki/SuperMemo_1.0_for_DOS).

In my [Master's Thesis](https://supermemo.guru/wiki/Master's_Thesis) (1990), I added a slightly more formal proof of the existence of the two components (see [next](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Proof)). That part of my thesis remained unnoticed.

In 1994, J. Kowalski [wrote in Enter](http://www.super-memory.com/articles/kowal.htm), Poland:

> We got to the point where the evolutionary interpretation of memory indicates that it works using the principles of increasing intervals and the [spacing effect](https://supermemo.guru/wiki/Spacing_effect). Is there any proof for this model of memory apart from the evolutionary speculation? In his Doctoral Dissertation, Wozniak discussed widely molecular aspects of memory and has presented a hypothetical model of changes occurring in the synapse in the process of learning. The novel element presented in the thesis was the distinction between the [stability](https://supermemo.guru/wiki/Stability) and [retrievability](https://supermemo.guru/wiki/Retrievability) of memory traces. This could not be used to support the validity of [SuperMemo](https://supermemo.guru/wiki/SuperMemo) because of the simple fact that it was SuperMemo itself that laid the groundwork for the hypothesis. However, an increasing molecular evidence seems to coincide with the stability-retrievability model providing, at the same time, support for the correctness of assumptions leading to SuperMemo. In plain terms, retrievability is a property of memory which determines the level of efficiency with which synapses can fire in response to the stimulus, and thus elicit the learned action. The lower the retrievability the less you are likely to recall the correct response to a question. On the other hand, stability reflects the history of earlier repetitions and determines the extent of time in which memory traces can be sustained. The higher the stability of memory, the longer it will take for the retrievability to drop to the zero level, i.e. to the level where memories are permanently lost. According to Wozniak, when we learn something for the first time we experience a slight increase in the stability and retrievability in synapses involved in coding the particular stimulus-response association. In time, retrievability declines rapidly; the phenomenon equivalent to forgetting. At the same time, the stability of memory remains at the approximately same level. However, if we repeat the association before retrievability drops to zero, retrievability regains its initial value, while stability increases to a new level, substantially higher than at primary learning. Before the next repetition takes place, due to increased stability, retrievability decreases at a slower pace, and the inter-repetition interval might be much longer before forgetting takes place. Two other important properties of memory should also be noted: (1) repetitions have no power to increase the stability at times when retrievability is high (spacing effect), (2) upon forgetting, stability declines rapidly

## Peer review publication (1995)

We published our ideas with Drs [Janusz Murakowski](https://supermemo.guru/wiki/Janusz_Murakowski) and [Edward Gorzelanczyk](https://supermemo.guru/wiki/Edward_Gorzelanczyk) in [1995](http://super-memory.com/english/2vm.htm). Murakowski perfected the mathematical proof. Gorzelanczyk fleshed out the molecular model. We have not heard much enthusiasm or feedback from the scientific community. The idea of two components of memory is like wine, the older it gets, the better it tastes. We keep wondering when it will receive a wider recognition. After all, we do not live in [Mendel](https://en.wikipedia.org/wiki/Gregor_Mendel)'s time to keep a good gem hidden in some obscure archive. There are [millions of users](https://supermemo.guru/wiki/Exponential_adoption_of_spaced_repetition) of [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition) and even if 0.1% got interested in the theory, they would hear of our two components. Today, even the [newest algorithm](https://supermemo.guru/wiki/Algorithm_SM-17) in [SuperMemo](https://supermemo.guru/wiki/SuperMemo) is based on the two-component model and it works like a charm. Ironically, users tend to flock to [simpler solutions](https://supermemo.guru/wiki/Algorithm_SM-2), where all the mechanics of human memory remain hidden. Even at [supermemo.com](http://supermemo.com/) we make sure we do not scare customers with excess numbers on the screen. This way we trade the instructional power of the model for better adoption among users who crave simplicity.

## Robert Bjork's research

The concept of the two components of memory has parallels in prior research, esp. by [Bjork](https://supermemo.guru/wiki/Bjork).

In the 1940s, scientists investigated habit strength and response strength as independent components of behavior in rats. Those concepts were later reformulated in Bjork's disuse theory. Herbert Simon seems to have noticed the [need for memory stability variable in his paper in 1966](https://supermemo.guru/wiki/Herbert_Simon_predicted_two_component_model_of_memory). In 1969, [Robert Bjork](https://supermemo.guru/wiki/Robert_Bjork) formulated the *Strength Paradox*: a reverse relationship between the probability of recall and the memory effect of a review. Note that his is a restatement of the [spacing effect](https://supermemo.guru/wiki/Spacing_effect) in terms of the [two component model](https://supermemo.guru/wiki/Two_component_model), which is just a short step away from formulating the distinction between the variables of memory. This led to [Bjork's New Theory of Disuse (1992)](https://www.researchgate.net/profile/Robert_Bjork/publication/281322665_A_new_theory_of_disuse_and_an_old_theory_of_stimulus_fluctuation/links/58b6f20945851591c5d55e96/A-new-theory-of-disuse-and-an-old-theory-of-stimulus-fluctuation.pdf) that would distinguish between the storage strength and the retrieval strength. Those are close equivalents of [retrievability](https://supermemo.guru/wiki/Retrievability) and [stability](https://supermemo.guru/wiki/Stability) with a slightly different interpretation of the mechanisms that underlie the distinction. Most strikingly, Bjork believes that when [retrievability](https://supermemo.guru/wiki/Retrievability) drops to zero, stable memories are still retained (in our model, stability becomes indeterminate). At the cellular level, Bjork might be right, at least for a while, but practise of [SuperMemo](https://supermemo.guru/wiki/SuperMemo) shows the power of [complete forgetting](https://supermemo.guru/wiki/Complete_forgetting), while, from the neural point of view, retaining memories in disuse would be highly inefficient independent of their [stability](https://supermemo.guru/wiki/Stability). Last but not least, Bjork defines storage strength in terms of connectivity, which is very close to what I believe happens in good students: [coherence](https://supermemo.guru/wiki/Coherence) affects [stability](https://supermemo.guru/wiki/Stability).

Why aren't the two components of memory entering mainstream research yet? I claim that if human mind tends to be short-sighted, and we all are, by design, the mind of science can be truly strangulated by strenuous duties, publish or perish, battles for grants, hierarchies, conflict of interest, [peer review](https://supermemo.guru/wiki/Peer_review), teaching obligations, and even the code of conduct. Memory researchers tend to live in a single dimension of "memory strength". In that dimension, they cannot truly appreciate true dynamics of molecular and neural processes that need to be investigated to crack the problem. Ironically, progress may come from those who tend to work in artificial intelligence or neural networks. Prodigious minds of [Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis) or [Andreas Knoblauch](https://scholar.google.de/citations?user=xVXueeQAAAAJ&hl=en) come up with twin ideas by independent reasoning process, models, and simulations. Biologists will need to listen to the language of mathematics and/or computer science.

## Two component model in Algorithm SM-17

The [two component model of long-term memory](https://supermemo.guru/wiki/Two_component_model_of_long-term_memory) underlies [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17). The success of Algorithm SM-17 is the ultimate practical proof for the correctness of the model.

A graph of actual changes in the value of the two components of memory provides a conceptual visualization of the evolving memory status:

[![SuperMemo: Changes in two variables of long-term memory](https://supermemo.guru/images/thumb/5/57/Memory_status.jpg/800px-Memory_status.jpg)](https://supermemo.guru/wiki/File:Memory_status.jpg)

> ***Figure:** Changes in memory status over time for an exemplary piece of knowledge. The horizontal axis represents time spanning the entire repetition history. The top panel shows retrievability (tenth power, R10, for easier analysis). Retrievability grid in gray is labelled by R=99%, R=98%, etc. The middle panel displays optimum intervals in navy. Repetition dates are marked by blue vertical lines and labelled in aqua. The end of the optimum interval where R crosses 90% line is marked by red vertical lines (only if intervals are longer than optimum intervals). The bottom panel visualizes stability (presented as ln(S)/ln(days) for easier analysis). The graph shows that retrievability drops fast (exponentially) after early repetitions when stability is low, however, it only drops from 100% to 94% in long 10 years after the 7th review. All values are derived from an actual repetition history and the three component model of memory.*

Due to the fact that a real-life application of [SuperMemo](https://supermemo.guru/wiki/SuperMemo) requires tackling learning material of varying difficulty, the third variable involved in the model is [item difficulty](https://supermemo.guru/wiki/Item_difficulty) (D). Some of the implications of item difficulty have also been discussed in [this article](https://supermemo.guru/wiki/History_of_spaced_repetition). In particular, the impact of composite memories with subcomponents of different memory stability (S).

For the purpose of the new algorithm we have defined the three components of memory as follows:

- [Memory stability](https://supermemo.guru/wiki/Memory_stability) (S) is defined as the [inter-repetition interval](https://supermemo.guru/wiki/Interval) that produces average [recall](https://supermemo.guru/wiki/Recall) probability of 0.9 at review time
- [Memory retrievability](https://supermemo.guru/wiki/Memory_retrievability) (R) is defined as the expected probability of [recall](https://supermemo.guru/wiki/Recall) at any time on the assumption of the [negatively exponential forgetting](https://supermemo.guru/wiki/Forgetting_curve) of homogenous learning material with the decay constant determined by [memory stability](https://supermemo.guru/wiki/Memory_stability) (S)
- [Item difficulty](https://supermemo.guru/wiki/Item_difficulty) (D) is defined as the maximum possible increase in [memory stability](https://supermemo.guru/wiki/Memory_stability) (S) at review mapped linearly into 0..1 interval with 0 standing for the easiest possible [items](https://supermemo.guru/wiki/Item), and 1 standing for the highest difficulty in consideration in [SuperMemo](https://supermemo.guru/wiki/SuperMemo) (the cut off limit currently stands at stability increase 6x less than the maximum recorded)

## Proof

The actual proof from my [Master's Thesis](https://supermemo.guru/wiki/Optimization_of_learning) follows. The language and models are 3 decades old and somewhat inept. However, the core idea still holds water to this day. For a better take on the proof, see [Murakowski proof](https://supermemo.guru/wiki/History_of_spaced_repetition_(print)#Proof_by_Murakowski).

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

**10.4.2. Two variables of memory: stability and retrievability**

There is an important conclusion that comes directly from the SuperMemo theory that there are two, and not one, as it is commonly believed, independent variables that describe the conductivity of a synapse and memory in general. To illustrate the case, let us again consider the calpain model of synaptic memory. It is obvious from the model, that its authors assume that only one independent variable is necessary to describe the conductivity of a synapse. Influx of calcium, activity of calpain, degradation of fodrin and number of glutamate receptors are all examples of such a variable. Note that all the mentioned parameters are dependent, i.e. knowing one of them we could calculate all others; obviously only in the case if we were able to construct the relevant formulae. The dependence of the parameters is a direct consequence of causal links between all of them.

However, the process of optimal learning requires exactly two independent variables to describe the state of a synapse at a given moment:

- A variable that plays the role of a clock that measures time between repetitions. Exemplary parameters that can be used here are:

  - Te - time that has elapsed since the last repetition (it belongs to the range <0,optimal-interval>),
  - Tl - time that has to elapse before the next repetition will take place (Tl=optimal-interval-Te),
  - Pf - probability that the synapse will lose the trace of memory during the day in question (it belongs to the range <0,1>).

- A variable that measures the durability of memory. Exemplary parameters that can be used here are:

  - I(n+1) - optimal interval that should be used after the next repetition (I(n+1)=I(n)*C where C is a constant greater than three),
  - I(n) - current optimal interval,
  - n - number of repetitions preceding the moment in question, etc.


Let us now see if the above variables are necessary and sufficient to characterize the state of synapses in the process of time-optimal learning. To show that variables are independent, we will show that none of them can be calculated from the other. Let us notice that the I(n) parameter remains constant during a given inter-repetition interval, while the Te parameter changes from zero to I(n). This shows that there is no function f that satisfies the condition:

> Te=f(I(n))

On the other hand, at the moment of subsequent repetitions, Te always equals zero while I(n) has always a different, increasing value. Therefore there is no function g that satisfies the condition:

> I(n)=g(Te)

Hence independence of I(n) and Te.

To show that no other variables are necessary in the process of optimal learning, let us notice that at any given time we can compute all the moments of future repetitions using the following algorithm:

1. Let there elapse I(n)-Te days.
2. Let there be a repetition.
3. Let Te be zero and I(n) increase C times.
4. Go to 1.

Note that the value of C is a constant characteristic for a given synapse and as such does not change in the process of learning. I will later use the term **retrievability** to refer to the first of the variables and the term **stability** to refer to the second one. To justify the choice of the first term, let me notice that we use to think that memories are strong after a learning task and that they fade away afterwards until they become no longer retrievable. This is retrievability that determines the moment at which memories are no longer there. It is also worth mentioning that retrievability was the variable that was tacitly assumed to be the only one needed to describe memory (as in the calpain model). The invisibility of the stability variable resulted from the fact that researchers concentrated their effort on a single learning task and observation of the follow-up changes in synapses, while the importance of stability can be visualized only in the process of repeating the same task many times. To conclude the analysis of memory variables, let us ask the standard question that must be posed in development of any biological model. What is the possible evolutionary advantage that arises from the existence of two variables of memory?

Retrievability and stability are both necessary to code for a process of learning that allows subsequent inter-repetition intervals to increase in length without forgetting. It can be easily demonstrated that such model of learning is best with respect to the survival rate of an individual if we acknowledge the fact that remembering without forgetting would in a short time clog up the memory system which is a finite one. If memory is to be forgetful it must have a means of retaining these traces that seem to be important for survival. Repetition as a memory strengthening factor is such a means. Let us now consider what is the most suitable timing of the repetitory process. If a given phenomenon is encountered for the n-th time, the probability that it will be encountered for the n+1 time increases and therefore a longer memory retention time seems advantageous. The exact function that describes the best repetitory process depends on the size of memory storage, number of possible phenomena encountered by an individual, and many others. However, the usefulness of increasing intervals required to sustain memory by repetitions is indisputable and so is the evolutionary value of retrievability and stability of memory. One can imagine many situations interfering with this simple picture of the development of memory in the course of evolution. For example, events that were associated with an intense stress should be remembered better. Indeed, this fact was proved in research on the influence of catecholamines on learning. Perhaps, using hormonal stimulation one could improve the performance of a student applying the SuperMemo method.

**Interim summary**

1. Existence of two independent variables necessary to describe the process of optimal learning was postulated. These variables were named [retrievability](https://supermemo.guru/wiki/Retrievability) and [stability](https://supermemo.guru/wiki/Stability) of memory
2. Retrievability of memory reflects the lapse of time between repetitions and indicates to what extent memory traces can successfully be used in the process of [recall](https://supermemo.guru/wiki/Recall)
3. Stability of memory reflects the history of repetitions in the process of learning and increases with each stimulation of the synapse. It determines the length of the [optimum inter-repetition interval](https://supermemo.guru/wiki/Optimum_interval)

[![Hypothetical mechanism involved in the process of optimal learning](https://supermemo.guru/images/thumb/0/0e/Hypothetical_mechanism_involved_in_the_process_of_optimal_learning.jpg/426px-Hypothetical_mechanism_involved_in_the_process_of_optimal_learning.jpg)](https://supermemo.guru/wiki/File:Hypothetical_mechanism_involved_in_the_process_of_optimal_learning.jpg)

> ***Figure:** In my Master's Thesis titled "Optimization of learning" (1990), I presented some hypothetical concepts that might underly the process of optimal learning based on spaced repetition. (A) Molecular phenomena (B) Quantitative changes in the synapse. Those ideas are a bit dated today, but the serrated curves representing memory retrievability came to be widely known in popular publications on spaced repetition. They are usually wrongly attributed to Hermann Ebbinghaus*

## Proof by Murakowski

Here is an improved proof by [Murakowski](https://supermemo.guru/wiki/Janusz_Murakowski):

> It has been found in earlier research that the [optimum spacing of repetitions](https://supermemo.guru/wiki/Spaced_repetition) in paired-associate learning, understood as the spacing which takes a minimum number of repetitions to indefinitely maintain a constant level of knowledge retention (e.g. 95%), can roughly be expressed using the following formulae ([Wozniak and Gorzelanczyk 1994](https://supermemo.guru/wiki/Optimization_of_repetition_spacing_in_the_practice_of_learning)):
>
> - (1) I1=C1
> - (2) Ii=Ii-1*C2
>
> where:
>
> - Ii - [inter-repetition interval](https://supermemo.guru/wiki/Optimum_interval) after the i-th repetition
> - C1 - length of the first interval (dependent on the chosen knowledge [retention](https://supermemo.guru/wiki/Retention), and usually equal to several days)
> - C2 - constant that denotes the increase of inter-repetition intervals in subsequent repetitions (dependent on the chosen knowledge retention, and the [difficulty](https://supermemo.guru/wiki/Difficulty) of the remembered item)
>
> The above formulae have been found for human subjects using computer optimization procedures employed to supervise the process of self-paced learning of word-pairs using the active recall drop-out technique. [...]
>
> As it will be shown below, the widely investigated strength of memory (or synaptic potentiation) does not suffice to account for the regular pattern of [optimum repetition spacing](https://supermemo.guru/wiki/Spaced_repetition): [...]
>
> 1. We want to determine the set of (molecular) variables involved in storing memory traces that will suffice to account for the optimum spacing of repetitions. Let us, initially, assume two correlates of these variables in learning that is subject to optimum spacing as expressed by Eqns. (1) and (2):
>
> 2. Just at the onset of the i-th repetition, *r*=0, while *s*i>*s*i-1>0 (*s*i denotes *s* right at the onset of the i-th repetition). This indicates that there is no function g1 such that s=g1(*r*), i.e. *s* cannot be a function of *r* only.
>
> 3. During the inter-repetition interval, *r*(t1)<>*r*(t2) if t1<>t2 (t denotes time and *r*(t) denotes *r* at the moment t). On the other hand, *s*(t1)=*s*(t2) (*s*(t) denotes *s* at the moment t). This shows that there is no function g2 such that *r*=g2(*s*), or we would have: *r*(t1)=g2(*s*(t1))=g2 (*s*(t2))=*r*(t2), which leads to a contradiction. *r* cannot be a function of *s* only.
>
> 4. In Steps 2 and 3 we have shown that *r* and *s* are independent, as there are no functions g1 and g2 such that *s*=g1(*r*) or *r*=g2(*s*). This obviously does not mean that there exists no parameter x and functions ys and yr such that *s*=ys(x) and *r*=yr(x).
>
> 5. It can be shown that
>
>     
>
>    r
>
>     
>
>    and
>
>     
>
>    s
>
>     
>
>    suffice to compute the optimum spacing of repetitions (cf. Eqns. (1) and (2)). Let us first assume that the two following functions f
>
>    r
>
>     
>
>    and f
>
>    s
>
>     
>
>    are known in the system involved in memory storage:
>
>     
>
>    r
>
>    i
>
>    =f
>
>    r
>
>    (
>
>    s
>
>    i
>
>    ) and
>
>     
>
>    s
>
>    i
>
>    =f
>
>    s
>
>    (
>
>    s
>
>    i-1
>
>    ). In our case, these functions have a trivial form f
>
>    r
>
>    :
>
>     
>
>    r
>
>    i
>
>    =
>
>    s
>
>    i
>
>     
>
>    and f
>
>    s
>
>    :
>
>     
>
>    s
>
>    i
>
>    =
>
>    s
>
>    i-1
>
>    *C
>
>    2
>
>     
>
>    (where C
>
>    2
>
>     
>
>    is the constant from Eqn. (2)). In such a case, the variables
>
>     
>
>    r
>
>     
>
>    and
>
>     
>
>    s
>
>     
>
>    are sufficient to represent memory at any moment t in optimum spacing of repetitions. Here is a repetition spacing algorithm which shows this to be true:
>
>    1. assume that the variables *r*i and *s*i describe the state of memory after the i-th repetition
>    2. let there elapse *r*i time
>    3. let there be a repetition
>    4. let the function fs be used to compute the new value of *s*i+1 from *s*i
>    5. let the function fr be used to compute the new value of *r*i+1 from *s*i+1
>    6. i:=i+1
>    7. goto 2
>
> The above reasoning shows that variables *r* and *s* form a sufficient set of independent variables needed to compute the optimum spacing of repetitions. Obviously, using a set of transformation functions of the form *r*’’=Tr(*r*’) and *s*’’=Ts(*s*’), one can conceive an infinite family of variable pairs *r*-*s* that could describe the status of the memory system. A difficult choice remains to choose such a pair *r*-*s* that will most conveniently correspond with molecular phenomena occurring at the level of the synapse.
>
> The following terminology and interpretation is proposed by the authors in a memory system involving the existence of the *r*-*s* pair of variables: the variable R, retrievability, determines the probability with which a given memory trace can be invoked at a given moment, while the variable S, stability of memory, determines the rate of decline of retrievability as a result of forgetting, and consequently the length of inter-repetition intervals in the optimum spacing of repetitions.
>
> Assuming the negatively exponential decrease of retrievability, and the interpretation of stability as a reciprocal of the retrievability decay constant, we might conveniently represent the relationship between R and S using the following formula (t denotes time):
>
> > (3) R=e-t/S
>
> The transformation functions from the pair *r*-*s* used in Steps 1-5 of the reasoning, to the proposed interpretation R-S will look as follows (assuming the definition of the optimum inter-repetition interval as the interval that produces retention of knowledge K=0.95):
>
> > (4) S=-*s*/ln(K)
> >
> > (5) R=e-(*s*-*r*)/S
>
> The relationship between the stability after the i-th repetition (Si) and the constants C1 and C2 determining the optimum spacing of repetitions as defined by Eqns. (1) and (2) can therefore be written as:
>
> > (6) Si=-(C1*C2i-1)/ln(K)
>
> and finally, retrievability in the optimum spacing of repetitions can be expressed as:
>
> > (7) Ri(t)=exp(t*ln(K)/(C1*C2i-1))
> >
> > where:
> >
> > - i - number of the repetition in question
> > - t - time since the i-th repetition
> > - Ri(t) - retrievability after the time t passing since the i-th repetition in optimum spacing of repetitions
> > - C1 and C2 - constants from Eqns. (1) and (2)
> > - K - retention of knowledge equal to 0.95 (it is important to notice that the relationship expressed by Eqn. (7) may not be true for retention higher than 0.95 due to the spacing effect resulting from shorter intervals)

## Two components of memory in SuperMemo

[SuperMemo](https://supermemo.guru/wiki/SuperMemo) has always been based on the [two component model of memory](https://supermemo.guru/wiki/Two_component_model_of_memory), which emerged in an increasingly explicit form over the last three decades. The constant C2 in Eqn. (2) in Murakowski proof above represents [stability increase](https://supermemo.guru/wiki/Stability_increase). In 2018, stability increase is represented in [SuperMemo](https://supermemo.guru/wiki/SuperMemo) as matrix *SInc[]*. C2 says how much inter-repetition intervals should increase in learning to meet the criteria of admissible level of forgetting. In reality, C2 is not a constant. It depends on a number of factors. Of these, the most important are:

- item difficulty (D)(see: [complexity](https://supermemo.guru/wiki/Complexity)): the more difficult the remembered piece of information the smaller the C2 (i.e. difficult material must be reviewed more often)
- [memory stability](https://supermemo.guru/wiki/Memory_stability) (S): the more lasting/durable the memory, the smaller the C2 value
- probability of recall ([retrievability](https://supermemo.guru/wiki/Retrievability))(R): the lower the probability of recall, the higher the C2 value (i.e. due to the [spacing effect](https://supermemo.guru/wiki/Spacing_effect), items are remembered better if reviewed with delay)

Due to those multiple dependencies, the precise value of C2 is not easily predictable. SuperMemo solves this and similar optimization problems by using multidimensional matrices to represent multi-argument functions and adjusting the value of those matrices on the basis of measurements made during an actual learning process. The initial values of those matrices are derived from a theoretical model or from previous measurements. The actually used values will, over time, differ slightly from those theoretically predicted or those derived from data of previous students.

For example, if the value of C2 for a given item of a given difficulty with a given memory status produces an inter-repetition interval that is longer than desired (i.e. producing lower than desired level of recall), the value of C2 is reduced accordingly.

Here is the evolution of [stability increase](https://supermemo.guru/wiki/Stability_increase) (constant C2) over years:

- in the paper-and-pencil version of SuperMemo (1985), C2 was indeed (almost) a constant. Set at the average of 1.75 (varying from 1.5 to 2.0 for rounding errors and simplicity), it did not consider [material difficulty](https://supermemo.guru/wiki/Difficulty), [stability](https://supermemo.guru/wiki/Stability) or [retrievability](https://supermemo.guru/wiki/Retrievability) of memories, etc.
- in early versions of [SuperMemo for DOS](https://supermemo.guru/wiki/SuperMemo_for_DOS) (1987), C2, named [E-Factor](https://supermemo.guru/wiki/E-Factor), reflected [item difficulty](https://supermemo.guru/wiki/Difficulty) for the first time. It was decreased for bad grades and increased for good grades
- SuperMemo 4 (1989) did not use C2, but, to compute inter-repetition intervals, it employed optimization matrices for the first time
- in [SuperMemo 5](https://supermemo.guru/wiki/SuperMemo_5) (1990), C2, named [O-Factor](https://supermemo.guru/wiki/O-Factor) was finally represented as a matrix and it included both the [difficulty](https://supermemo.guru/wiki/Difficulty) dimension as well as the [stability](https://supermemo.guru/wiki/Stability) dimension. Again, entries of the matrix would be subject to the measure-verify-correct cycle that would, starting with the initial value based on prior measurements, produce a convergence towards the value that would satisfy the learning criteria
- in [SuperMemo 6](https://supermemo.guru/wiki/SuperMemo_6) (1991), C2, in the form of the [O-Factor matrix](https://supermemo.guru/wiki/O-Factor_matrix) would be derived from a three-dimensional matrix that would include the [retrievability](https://supermemo.guru/wiki/Retrievability) dimension. The important implication of the third dimension was that, for the first time, [SuperMemo](https://supermemo.guru/wiki/SuperMemo) would make it possible to inspect [forgetting curves](https://supermemo.guru/wiki/Forgetting_curve) for different levels of [difficulty](https://supermemo.guru/wiki/Difficulty) and [memory stability](https://supermemo.guru/wiki/Stability)
- in [SuperMemo 8](https://supermemo.guru/wiki/SuperMemo_8) (1997) through [SuperMemo 16](http://www.super-memo.com/supermemo16.html), the representation of C2 would not change much, however, the algorithm used to produce a quick and stable transition from the theoretical to the real set of data would gradually get more and more complex. Most importantly, new SuperMemos make a better use of the [retrievability](https://supermemo.guru/wiki/Retrievability) dimension of C2. Thus, independent of the [spacing effect](https://supermemo.guru/wiki/Spacing_effect), the student can depart from the initial learning criteria, e.g. to cram before an exam, without introducing noise into the optimization procedure
- in [SuperMemo 17](https://supermemo.guru/wiki/SuperMemo_17) (2016), C2 finally took the form based on the original [two component model of memory](https://supermemo.guru/wiki/Two_component_model_of_memory). It is taken from [stability increase](https://supermemo.guru/wiki/Stability_increase) matrix (SInc) that has three dimensions that represent the three variables that determine the increase in stability: [complexity](https://supermemo.guru/wiki/Complexity), [stability](https://supermemo.guru/wiki/Stability) and [retrievability](https://supermemo.guru/wiki/Retrievability). The SInc matrix is filled up with data during learning using a complex algorithm known as [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17). The stability increase matrix can be inspected in SuperMemo 17 with **Tools : Memory : 4D Graphs** (**Stability** tab)